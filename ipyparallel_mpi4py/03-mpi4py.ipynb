{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Message Passing Interface MPI\n",
    "## Introduction  to MPI\n",
    "The Message Passing Interface (MPI) is:\n",
    "\n",
    "- Particularly useful for distributed memory machines\n",
    "- The _de facto_ standard parallel programming interface\n",
    "\n",
    "Many implementations exist - MPICH, OpenMPI, ...\n",
    "\n",
    "Interfaces in \n",
    "- C/C++ \n",
    "- Fortran and ... \n",
    "- Python wrappers (MPI4Py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Message Passing Paradigm\n",
    "The parallel program is launched as separate processes (tasks) each with their own address space.\n",
    "- To achieve parallelism we should partition data across tasks \n",
    "\n",
    "Data must be **explicitly moved** from process to process:\n",
    "- A task can access the data of another process through passing a message (a copy of the data is passed from one process to another)\n",
    "\n",
    "Two main classes of message passing:\n",
    "- **Point-to-point** operations, involving only two processes\n",
    "- **Collective** operations, involving a group of processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MPI4Py\n",
    "MPI4Py provides an interface similar to the MPI standard C++ interface\n",
    "\n",
    "You can communicate Python objects\n",
    " - e.g. entire numpy arrays, rather than splitting into data and metadata that surrounds the object\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Communicators\n",
    "MPI uses communicator objects to identify a set of processes that can communicate with each other\n",
    "\n",
    "- `MPI_COMM_WORLD` is a default communicator, which contains all processes\n",
    "\n",
    "Processes have ranks \n",
    "- Unique process id in a given communicator, assigned by the system when the process initializes\n",
    "- Used to specify the sources and destinations of messages\n",
    "- 0 is the \"root process\", often used for I/O, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hello World!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%file hello_world_mpi.py\n",
    "#import the MPI class from mpi4py\n",
    "from mpi4py import MPI\n",
    "# call the COMM_WORLD attribute, store that in comm\n",
    "comm = MPI.COMM_WORLD\n",
    "# one of the attributes comm has is rank\n",
    "print(\"Hello world, I am process: \" + str(comm.rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "! srun -n 2 python hello_world_mpi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point to Point Communication\n",
    "\n",
    "Point-to-point communication is sending message/data from one process to another. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%file send_and_receive.py\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "size = comm.size\n",
    "a = numpy.array([rank]*10, dtype=float)\n",
    "if rank == 0:\n",
    "    comm.send(a, dest = (rank + 1) % size)\n",
    "if rank > 0:\n",
    "    data = comm.recv(source = (rank - 1) % size)\n",
    "    comm.send(a, dest = (rank + 1) % size)\n",
    "if rank == 0:\n",
    "    data = comm.recv(source = size - 1)\n",
    "print(\"My ranks is \" + str(rank) + \"\\n and I received this array:\\n\" + str(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "! srun -n 8 python send_and_receive.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collective communication \n",
    "Generally groups of processes need to exchange messages between themselves. Rather than explicitly sending and receiving such messages from point to point, MPI comes with group operations known as collectives.\n",
    "- Broadcast, scatter, gather and reduction\n",
    "- Implementations can optimize performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Broadcast\n",
    "Send from one process to all other processes.\n",
    "\n",
    "![broadcast](03-broadcast.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scatter\n",
    "Split data into chunks and send a chunk to individual processes to work on.\n",
    "\n",
    "![scatter](03-scatter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gather\n",
    "Gather the chunks and bring them to the root process\n",
    "\n",
    "![gather](03-gather.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reduction\n",
    "Gather, and do some computation.\n",
    "\n",
    "![reduce](03-reduce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scatter\n",
    "We create an array on rank 0 and scatter it to all ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%file scatter.py\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "comm = MPI.COMM_WORLD\n",
    "sendbuf = []\n",
    "if comm.rank == 0:\n",
    "    m = numpy.random.randn(comm.size, comm.size)\n",
    "    print(\"Original array on root process\\n\" + str(m))\n",
    "    sendbuf = m\n",
    "# call this on every rank, including rank 0\n",
    "v = comm.scatter(sendbuf, root=0)\n",
    "print(\"I got this data:\\n\" + str(v) + \"\\n and my ranks is \" + str(comm.rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "! srun -n 4 python scatter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gather\n",
    "Collect the results from all processes onto rank 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%file gather.py\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "comm = MPI.COMM_WORLD\n",
    "sendbuf = []\n",
    "\n",
    "if comm.rank == 0:\n",
    "    m = numpy.array(range(comm.size * comm.size), dtype=float)\n",
    "    m.shape=(comm.size, comm.size)\n",
    "    print(\"Original array on root process\\n\" + str(m))\n",
    "    sendbuf = m\n",
    "    \n",
    "# first scatter like before\n",
    "v = comm.scatter(sendbuf, root=0)\n",
    "print(\"I got this data:\\n\" + str(v) + \"\\n and my ranks is \" + str(comm.rank))\n",
    "#do some work on each process and then gather back onto root\n",
    "v = v * v\n",
    "recvbuf = comm.gather(v, root=0)\n",
    "if comm.rank == 0:\n",
    "        print(\"New array on rank 0:\\n\" + str(numpy.array(recvbuf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "! srun -n 4 python gather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reduction\n",
    "Create an array, scatter it, and do a parallel reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%file reduce.py\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "comm = MPI.COMM_WORLD\n",
    "sendbuf = []\n",
    "\n",
    "if comm.rank == 0:\n",
    "    m = numpy.array(range(comm.size * comm.size), dtype=float)\n",
    "    m.shape=(comm.size, comm.size)\n",
    "    print(\"Original array on root process\\n\" + str(m))\n",
    "    sendbuf = m\n",
    "    \n",
    "# first scatter like before\n",
    "v = comm.scatter(sendbuf, root=0)\n",
    "print(\"I got this data:\\n\" + str(v) + \"\\n and my ranks is \" + str(comm.rank))\n",
    "\n",
    "recvbuf = comm.reduce(v, root=0)\n",
    "if comm.rank == 0:\n",
    "        print(\"New array on rank 0:\\n\" + str(numpy.array(recvbuf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "! srun -n 4 python reduce.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning alert-block alert-info\"><b>Exercise:</b> Find the total sum by calling numpy.sum on the final array</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Capitalized vs lower case versions\n",
    "In Python there are two versions of the various MPI methods:\n",
    "- Upper case (Send, Recv, Gather, etc.) \n",
    "- lower case (send, recv, gather, etc.)\n",
    "\n",
    "To use the upper-case version of the methods the data object must support Python's \"single-segment buffer interface\". This is a standard Python mechanism provided by some types e.g., numerical arrays and strings.\n",
    "\n",
    "You can transmit arbitrary Python data types using the lower-case version of the methods. MPI4py will serialize the data type, send it to the remote process, then deserialize it back to the original data type (a process known as pickling and unpickling). This adds significant overhead to the MPI operation.\n",
    "\n",
    "Use the upper-case versions where possible!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scatter with upper case-version \n",
    "We create an array on rank 0 and scatter the rows to the ranks. Replace <TO_DO> with functioning calls.\n",
    "<div class=\"alert alert-warning alert-block alert-info\"><b>Exercise:</b> Replace <TO_DO> with functioning calls.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%file scatter_uppercase.py\n",
    "<TO_DO>\n",
    "import numpy as np\n",
    "\n",
    "comm = <TO_DO>\n",
    "rank = <TO_DO>\n",
    "size = <TO_DO>\n",
    "\n",
    "A = np.zeros((size,size))\n",
    "if rank==0:\n",
    "    A = np.random.randn(size, size)\n",
    "    print(\"Original array on root process\\n\", A)\n",
    "local_a = np.zeros(size)\n",
    "\n",
    "comm.<TO_DO>(A, local_a, root=0)\n",
    "print(\"Process\", rank, \"received\", local_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "! srun -n 4 python scatter_uppercase.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using MPI4py in a Notebook\n",
    "\n",
    "To use MPI directly in a Jupyter Notebook you need to combine:\n",
    "\n",
    "- mpi4py, and\n",
    "- IPython Parallel \n",
    "\n",
    "The IPython Parallel engines need to be started using the `mpirun` command (or equivalent). On our system:\n",
    "\n",
    "- Start the **ipcontroller** \n",
    "- Start the **ipengines** using `srun` and with `--mpi` argument.\n",
    "\n",
    "You can then use the parallel magics, `%px` and `%%px`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Scatter with upper case-version \n",
    "<div class=\"alert alert-warning alert-block alert-info\"><b>Exercise:</b> Perform the previous scatter directly in a notebook using the %%px magic. You will need to start the ipcontroller and engines as described above, import ipyparallel and start a client.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "miniconda-pythonhpc",
   "language": "python",
   "name": "miniconda-pythonhpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
