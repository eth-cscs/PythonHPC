{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Stochastic Gradient Descent in Two Nodes with TensorFlow, Horovod and IPCMagic.\n",
    "\n",
    "Here we visualize the minimization of the loss with the SGD algorithm.\n",
    "For this, we consider a linear model with only two weights (the slope and the offset).\n",
    "\n",
    "With this example we show how use IPCMagic to run TensorFlow in two nodes from a Jupyter notebook.\n",
    "We consider blocking and non-blocking execution of code on the IPCluster.\n",
    "We also show how to combine non-blocking execution and the function `ipcmagic.utilities.watch_asyncresult` to have real time loging during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipcmagic\n",
    "import ipyparallel as ipp\n",
    "from ipcmagic import utilities   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ipcluster --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ipcluster start -n 2 --mpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import socket\n",
    "socket.gethostname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import horovod.tensorflow.keras as hvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "hvd.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 0\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# Create a linear function with noise as our data\n",
    "nsamples = 1000\n",
    "ref_slope = 2.0\n",
    "ref_offset = 0.0\n",
    "noise = np.random.random((nsamples, 1)) - 0.5    # -0.5 to center the noise\n",
    "x_train = np.random.random((nsamples, 1)) - 0.5  # -0.5 to center x around 0\n",
    "y_train = ref_slope * x_train + ref_offset + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train.astype(np.float32),\n",
    "                                              y_train.astype(np.float32)))\n",
    "dataset = dataset.shuffle(1000)\n",
    "dataset = dataset.batch(100)\n",
    "dataset = dataset.repeat(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(1, input_shape=(1,), activation='linear'),\n",
    "])\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(lr=0.5)\n",
    "opt = hvd.DistributedOptimizer(opt)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "class TrainHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.vars = []\n",
    "        self.loss = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.vars.append([v.numpy() for v in self.model.variables])\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        \n",
    "history = TrainHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "initial_sync = hvd.callbacks.BroadcastGlobalVariablesCallback(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "# # blocking execution.\n",
    "# # the output is visible only at the end of the run\n",
    "\n",
    "fit = model.fit(dataset, callbacks=[initial_sync, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --noblock -o training\n",
    "\n",
    "# the non-blocking execution (`%%px --noblock`) returns an `AsyncResult` object inmediately.\n",
    "# the `AsyncResult` object can be accessed from python with the option `-o <variable>`.\n",
    "# by doing that we can fetch information while the code running.\n",
    "\n",
    "fit = model.fit(dataset, callbacks=[initial_sync, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch the output in real time\n",
    "utilities.watch_asyncresult(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 0\n",
    "slope_hist = np.array(history.vars)[:, 0]\n",
    "offset_hist = np.array(history.vars)[:, 1]\n",
    "loss_hist = np.array(history.loss)\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (8, 3)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_hist[10:], 'r.-')\n",
    "plt.xlabel('Training steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_train, y_train, '.')\n",
    "plt.plot(x_train, slope_hist[-1] * x_train + offset_hist[-1], 'r-')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 0\n",
    "def loss_function_field(m, n, xref, yref):\n",
    "    '''Utility function for ploting the loss'''\n",
    "    return np.mean(np.square(yref - m * xref - n ))\n",
    "\n",
    "_m = np.arange(-0.0, 4.01, 0.1)\n",
    "_n = np.arange(-0.5, 0.51, 0.1)\n",
    "M, N = np.meshgrid(_m, _n)\n",
    "\n",
    "Z = np.zeros(M.shape)\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        Z[i, j] = loss_function_field(M[i, j], N[i, j],\n",
    "                                      x_train, y_train)\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 7)\n",
    "\n",
    "cp = plt.contour(M, N, Z, 15, vmin=Z.min(), vmax=Z.max(), alpha=0.99, colors='k', linestyles='--')\n",
    "plt.contourf(M, N, Z, vmin=Z.min(), vmax=Z.max(), alpha=0.8, cmap=plt.cm.RdYlBu_r)\n",
    "plt.clabel(cp, cp.levels[:6])\n",
    "plt.colorbar()\n",
    "m = slope_hist[-1]\n",
    "n = offset_hist[-1]\n",
    "plt.plot(slope_hist, offset_hist, '.-', lw=2, c='k')\n",
    "plt.plot([ref_slope], [ref_offset], 'rx', ms=10)\n",
    "plt.xlim([_m.min(), _m.max()])\n",
    "plt.ylim([_n.min(), _n.max()])\n",
    "plt.xlabel('Slope')\n",
    "plt.ylabel('Offset')\n",
    "plt.show()\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ipcluster stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyhpc-tf2",
   "language": "python",
   "name": "pyhpc-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
