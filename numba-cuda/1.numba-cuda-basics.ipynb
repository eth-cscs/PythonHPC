{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Numba for GPUs\n",
    "\n",
    "*Vasileios Karakasis, **Theofilos Manitaras**, CSCS*\n",
    "\n",
    "You may use Numba to generate GPU code from high-level Python functions. Both CUDA and ROCm GPUs (NVIDIA and AMD, respectively) are supported, but we are going to focus only on the CUDA capabilities of Numba.\n",
    "\n",
    "There is an almost one-to-one mapping between the Numba high-level Python abstractions and the different CUDA constructs. This practically means that, as a programmer, you need to take care explicitly of the host/device memory management and you need to be aware of the CUDA programming model.\n",
    "\n",
    "This demo will teach you to write your first CUDA kernels using Numba. It will cover the basic CUDA programming principles, but it should be enough to kick start you in GPU programming. More specifically, we will cover the following topics:\n",
    "\n",
    "- Writing a GPU kernel\n",
    "- Moving data to/from the GPU.\n",
    "- Spawning a GPU kernel\n",
    "- Profiling a GPU kernel\n",
    "- Optimizing memory accesses\n",
    "- Making use of the shared memory\n",
    "\n",
    "We will not cover CUDA streams.\n",
    "\n",
    "## Verify that Numba sees the GPU and understands CUDA\n",
    "\n",
    "First thing is to check if Numba can detect the GPU. You can achieve this by running the `numba` executable that comes with Numba's installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!numba -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this command does not work out of the box for your Numba installation, you may have to set the `CUDA_HOME` environment variable to point to your CUDA installation.\n",
    "\n",
    "## Writing the first kernel\n",
    "\n",
    "Here is a vector addition kernel that takes two vectors as input and writes the sum in a third vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba.cuda as cuda\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def _vecadd_cuda(z, x, y):\n",
    "    i = cuda.grid(1)\n",
    "    N = x.shape[0]\n",
    "    if i >= N:\n",
    "        return\n",
    "\n",
    "    z[i] = x[i] + y[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty simple, right? Let's explain each line of this code.\n",
    "\n",
    "The `@cuda.jit` decorator will compile the following function into a CUDA kernel at runtime. We will see the cost of it later on.\n",
    "\n",
    "A CUDA kernel in Numba is a Python function that does *not* return a value. This is in accordance with CUDA, where kernel functions are declared `void`. The arguments of the function can be either Numpy arrays or scalars of a Numba recognized type.\n",
    "\n",
    "CUDA kernels specify the work to be done by a single GPU thread. Due to the massive hardware parallelism available on the GPU, a single GPU thread gets only a tiny portion of work, in this case, the sumation of a single element in the vectors. Since each element is independent from each other, we can safely create as many threads as the elements of the target vectors and spawn them (we will see how later).\n",
    "\n",
    "Threads on the GPU as organized in groups, called CUDA blocks. There are constraints on the maximum number of threads a block can contain. For the P100 GPUs on Daint, this is 1024. This means that you will need multiple blocks to calculate the sum of large vectors. The blocks that comprise a kernel form the *grid*. Threads inside a block are numbered sequentially and the blocks inside the grid are numbered, too. The figure below shows the arrangement of threads for the vector addition example.\n",
    "\n",
    "![Arrangement of threads in CUDA blocks](figs/cuda-blocks.png)\n",
    "\n",
    "In order to obtain the i-th element of the vector given a block size $B$, you would have to calculate the following:\n",
    "\n",
    "\\begin{equation}\n",
    "i = i_{b}B + i_{t}\n",
    "\\end{equation}\n",
    "\n",
    "where $i_{b}$ is the block index and $i_{t}$ is the thread index. CUDA provides this information and Numba makes it available, so that the above statement would be written as follows:\n",
    "\n",
    "```python\n",
    "i = cuda.blockIdx.x*cuda.blockDim.x + threadIdx.x\n",
    "```\n",
    "\n",
    "Blocks and grids can be three dimensional, thus the `.x` attribute in all of these variables (the other dimensions are accessed through the `.y` and `.z` attributes).\n",
    "\n",
    "Since obtaining the absolute position of a thread in a CUDA is quite common operation, Numba offers the convenience function `grid()`, which essentially does automatically the above calculation:\n",
    "\n",
    "```python\n",
    "i = cuda.grid(1)\n",
    "```\n",
    "\n",
    "The argument passed to the `grid()` function is the number of dimensions of the grid.\n",
    "\n",
    "The next three statements in the code simply check that we don't overrun the arrays in case that their dimension is not a multiple of the block size. In this case, some threads of the last block will remain idle:\n",
    "\n",
    "```python\n",
    "    N = x.shape[0]\n",
    "    if i >= N:\n",
    "        return\n",
    "```\n",
    "\n",
    "> The threads inside a block are run in batches of 32 at once, called *warps*. All the threads of the warp execute the same instruction. If the program control flow diverges for some of the threads of the warp, e.g., due to an `if` condition, the branches will be executed sequentially by disabling the non participating threads. This condition is called *warp divergence* and may incur a performance penalty.\n",
    "\n",
    "Finally, the `z[i] = x[i] + y[i]` computes the actual sum.\n",
    "\n",
    "\n",
    "## Preparing the data for the GPU\n",
    "\n",
    "All Numba CUDA kernels operate on data residing on the GPU. That means that the `x`, `y` and `z` arrays must be  transferred from the host (CPU) to the device (accelerator) before calling the CUDA kernel.\n",
    "\n",
    "Let's create first two vectors on the host:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "N = 1024\n",
    "x = rng.random(N)\n",
    "y = rng.random(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute our reference result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_ref = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to transfer the vectors to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `d_x` and `d_y` are Numpy *array-like* objects that have their data mapped in the GPU memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `z` array, we don't need to create it on the host and copy it, since it is essentially an output only array. We can simply allocate it directly on the GPU and copy it out when the kernel finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_z = cuda.device_array_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will simply allocate an array like `x`, i.e., with the same element type, shape and order, on the GPU.\n",
    "\n",
    "Data is ready, now it's time to call the kernel!\n",
    "\n",
    "## Invoking a CUDA kernel\n",
    "\n",
    "When invoking a CUDA kernel you have to specify the block size to use and the corresponding grid size. Picking the right size of block is not always straightforward, but usually values between 64 and 256 are good enough. \n",
    "\n",
    "> The block size has a direct effect on the *occupancy* of each GPU SM, i.e., how much the actual hardware threads of the SM are utilized, and as a result to the occupancy of the whole GPU. Performance-wise, though, it does not have such a big impact. Nvidia provides a nice tool for calculating the occupancy of the GPU, that you can find [here](https://docs.nvidia.com/cuda/cuda-occupancy-calculator/CUDA_Occupancy_Calculator.xls).\n",
    "\n",
    "For our kernel, we will select a block size of 128 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having decided the block size we need to set up the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks = N // block_size\n",
    "if N % block_size:\n",
    "    num_blocks += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to call the kernel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_vecadd_cuda[num_blocks, block_size](d_z, d_x, d_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the block and the grid could have been two-dimensional (not in this example), in which case you would just define them as tuples.\n",
    "\n",
    "## Copying back the results\n",
    "\n",
    "As mentioned before, kernels operate on GPU data only. We need a way to transfer back to the host the result, which is the `d_z` array. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = d_z.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's validate the result though to make sure that everything has worked fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(z_ref, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "For completeness and easy reference, here is the whole vector addition example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def _vecadd_cuda(z, x, y):\n",
    "    '''The CUDA kernel'''\n",
    "    i = cuda.grid(1)\n",
    "    N = x.shape[0]\n",
    "    if i >= N:\n",
    "        return\n",
    "\n",
    "    z[i] = x[i] + y[i]\n",
    "\n",
    "\n",
    "# Set up the host vectors\n",
    "N = 1000\n",
    "x = rng.random(N)\n",
    "y = rng.random(N)\n",
    "\n",
    "# Copy and allocate data on the device\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "d_z = cuda.device_array_like(x)\n",
    "\n",
    "# Set up the kernel invocation\n",
    "block_size = 128\n",
    "num_blocks = N // block_size\n",
    "if N % block_size:\n",
    "    num_blocks += 1\n",
    "\n",
    "# Call the kernel\n",
    "_vecadd_cuda[num_blocks, block_size](d_z, d_x, d_y)\n",
    "\n",
    "# Copy back the result to the host\n",
    "res = d_z.copy_to_host()\n",
    "\n",
    "# Validate the result\n",
    "assert np.allclose(x + y, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "> 1. Make the array sufficiently large and time the Numpy version of the sum `x + y`.\n",
    "> 2. Now time the call to the CUDA kernel with `%timeit -n1 -r1`. What do you see?\n",
    "> 3. Try timing the CUDA kernels with `%timeit -n1 -r2`. What is happening?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
